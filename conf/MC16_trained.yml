#!/usr/bin/env python3
# -*- coding: utf-8 -*-
runner:
    ctrl_min_amp: -0.04285714285714286
    ctrl_max_amp: 0.04285714285714286
    
    partial_reward: False

    nb_interactions: 3000
    
    nb_episodes : 100

    train_steps : 300
    gradient_steps : 64

    RL_algorithm : 'DDPG'
    custom_policy : True
    policy_file : '../conf/default_custom_policy.yml'

    agent_run_name: 1672737751

    normalize_input : "utau"

    evaluation: True
    rewrite_input_files: True
    learnt_policy: True

    policy: 'rl_model_282624000_steps'

simulation:
    init_field: '../../../data/baseline/init_16x65x16_minchan_000.u'

    dt : 0.007 

    xl : 5.34
    varsiz: False

    wbci: 7

    nctrlz: 16
    nctrlx: 16
    zchange: 1.
    xchange: 1.

    ixys : 512
    ixyss : 1024

    npl: 3

    ndrl: 80

    nproc: 4
    nx: 16
    ny: 65
    nz: 16

    nxjet: 1
    nzjet: 1
    nxs: 16
    nzs: 16
